# Chapter 6: Scaling-DEIT

[Scaling-DEIT: Scaling up Data-Efficient Image Transformer (2021)](TODO.md)

### Core Learning Takeaway :
Learning how to train Vision-Transformers on Multiple GPU's
- via methods such as (a) data parallelism, (b) gradient accumulation, (c) mixed precision training, and (d) sync batch normalization.

____

## A  - Weekly Deliverables :

1. Read [Scaling up Data-Efficient Image Transformer](TODO.md)Paper
2. Compose a Report/Summary of the Paper - [My S-DEIT Notes](TODO.md)
3. [Code up the Model Specified in the paper, Train, and Share the Results](TODO.md)

4. Compose Weekly Progress Report : 
```
  a - Previous Weeks Deliverables
  b - Deliverables Completed from the Previous Week
  c - Next Weeks Deliverables
```
