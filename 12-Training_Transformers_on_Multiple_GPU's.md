# Chapter 12: Training Transformer-based Language Models on Multi-GPUs and TPU

[Training Transformer-based Language Models on Multi-GPUs and TPUs (2019)](TODO.md)

### Core Learning Takeaway :

Provides a method for training large models on multiple GPUs using model parallelism, where different parts of the model are distributed across different GPUs, 
- gain an understand of how to train large models on a single GPU.

____

## A  - Weekly Deliverables :

1. Read [Training Transformer-based Language Models on Multi-GPUs and TPUs](TODO.md)Paper
2. Compose a Report/Summary of the Paper - [My "Training Transformer-based Language Models on Multi-GPUs" Notes](TODO.md)
3. [Code up the Model Specified in the paper, Train, and Share the Results](TODO.md)

4. Compose Weekly Progress Report : 
```
  a - Previous Weeks Deliverables
  b - Deliverables Completed from the Previous Week
  c - Next Weeks Deliverables
```

## Optional reading
[Training deep neural networks on supercomputers (2018)](TODO.md)