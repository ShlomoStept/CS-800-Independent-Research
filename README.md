# CS-800-Independent-Research Curriculum

## A - Course Weekly Deliverables 

1. Read Chapter's Assigned Paper
2. Compose a Report/Summary of the Paper - ()
3. Code up the Model Specified in the paper, Train, and Share the Results 

**WARNING**: Some Models will take longer to train than others. 

4. Compose Weekly Progress Report : 
```
  a - Previous Weeks Deliverables
  b - Deliverables Completed from the Previous Week
  c - Next Weeks Deliverables
```
_____

## B - Curriculum
The curriculum is divided into 12 chapters - each of which are intended to be completed in 1 week :

**WARNING**:At Week 8 - an assesment of the weekly progresion will be made, to update the curriculum plan according to the pace 

## 16 Week Plan : *(~ 1 Chapter per week)*
- [Chapter 0: Auto Differentiation](0-Auto_Differentiation.md)
- [Chapter 1: Transformers](1-Transformers.md)
- [Chapter 2: Transformers-XL](2-Transformers_XL.md)
- [Chapter 3: BERT: Bidirectional Transformer](3-BERT.md)
- [Chapter 4: VIT: First Vision Transformer](4-VIT.md)
- [Chapter 5: DEIT: Data Efficent Image Transformer](5-DEIT.md)
- *[Chapter 6: S-DEIT: Scaling DEIT](6-S-DEIT.md)* *~May Skip depending on Progress~*
- [Chapter 7: Efficent-Net](7-Efficent-Net.md)
- [Chapter 8: SWIN-Transformer: Scaled Window image Transformer](8-SWIN-Transformer.md)
- *[Chapter 9: SWIN-II](9-SWIN-II.md)* *~May Skip depending on Progress~*
- [Chapter 10: Scaling Laws For Neural Network Models.md](10-Scaling_Laws_For_Neural_Network_Models.md)
- [Chapter 11: Training Large Batch Sizes with Data Parallelism](11-Training_Large_Batch_Sizes_with_Data_Parallelism.md)
- [Chapter 12: Training Transformers on Multiple GPU's](12-Training_Transformers_on_Multiple_GPU's.md)
- [Chapter 13-16: Implimenting My Model: Octagonal Attention](13-Octagonal_Attention.md)

_____

### C - Each chapter has three main sections:

1. **Main Paper**: A single Main Paper will be singled out, to be studied in depth and implimented. Must read in enteirty and compose summart/deep-dive. 
2. **Summary of Paper** My personal summary/notes on the Main Paper.
3. **Code/Model Implimentation**: An implimentation of the model described by the Main Paper.

- **BONUS: Optional reading**: Related material that's still very relevant to the chapter, but can be either skimmed or skipped entirely and used as a reference later on.


_____

## D - Usefull Sources

TextBooks:

- [Goodfellow et al](https://www.deeplearningbook.org/) - Probably still the best deep learning-specific textbook, although it can be unclear in places. Has no exercises.
- [Jared Kaplan's notes for physicists](https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf) - An extended introduction to deep learning from a fairly theoretical perspective. Also has no exercises.

_____


Online courses:

- [Deep Learning Specialization from deeplearning.ai](https://www.deeplearning.ai/program/deep-learning-specialization/) - A much longer extension of the machine learning Coursera course linked above.
- [Deep Learning Udacity course](https://www.udacity.com/course/deep-learning-nanodegree--nd101) - Another longer deep learning course.

## E - Advice to myself

These are miscellaneous opinions based on personal experience, so take them with a pinch of salt. [An Opinionated Guide to ML Research](http://joschu.net/blog/opinionated-guide-ml-research.html) is also worth a read.
