# Chapter 2: Transformers-XL

[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)](TODO.md)

### Core Learning Takeaway :

How to handle longer-term dependencies by using a relative position representation.

____

## A  - Weekly Deliverables :

1. Read [Transformer-XL](TODO.md) Paper
2. Compose a Report/Summary of the Paper - [My Transformer-XL Notes](TODO.md)
3. [Code up the Model Specified in the paper, Train, and Share the Results](TODO.md)

4. Compose Weekly Progress Report : 
```
  a - Previous Weeks Deliverables
  b - Deliverables Completed from the Previous Week
  c - Next Weeks Deliverables
```
