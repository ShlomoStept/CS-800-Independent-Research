# Chapter 3: BERT

[BERT: Bidirectional Encoder Representations from Transformers (2018)](TODO.md)

### Core Learning Takeaway :

How to use a bidirectional self-attention mechanism to learn contextual representations of words.
The importance and usefullness of pretraining


____

## A  - Weekly Deliverables :

1. Read [Bidirectional Encoder Representations from Transformers](TODO.md) Paper
2. Compose a Report/Summary of the Paper - [My BERT Notes](TODO.md)
3. [Code up the Model Specified in the paper, Train, and Share the Results](TODO.md)

4. Compose Weekly Progress Report : 
```
  a - Previous Weeks Deliverables
  b - Deliverables Completed from the Previous Week
  c - Next Weeks Deliverables
```
